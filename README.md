# π§Hansol_llm_fine-tuning
λ„λ°° ν•μ μ§μ μ‘λ‹µ μ²λ¦¬ : ν•μ†”λ°μ½” μ‹μ¦2 AI κ²½μ§„λ€ν
[![λ€νμ •λ³΄](./pngs/dacon.png)](https://dacon.io/competitions/official/236216/overview/description)  
- λ€ν λ°”λ΅κ°€κΈ° μ΄λ―Έμ§€ ν΄λ¦­
- μ΄λ² μ‹μ¦ 2μ—μ„λ” ν• κ±Έμ λ” λ‚μ•„κ°€, NLP(μμ—°μ–΄ μ²λ¦¬) κΈ°λ°μ QA (μ§λ¬Έ-μ‘λ‹µ) μ‹μ¤ν…μ„ ν†µν•΄ λ„λ°°ν•μμ™€ κ΄€λ ¨λ κΉμ΄ μλ” μ§μμ‘λ‹µ μ²λ¦¬ λ¥λ ¥μ„ κ°–μ¶ AI λ¨λΈ κ°λ°μ— λ„μ „ν•©λ‹λ‹¤.
## Private μμ„  
![μμ„](pngs/leader-board.png)


## Member   
### 
  **μµμΉν**  
  [Dacon](https://dacon.io/myprofile/472402/home)  
  [Github](https://github.com/ColdTbrew)  
  [Hugging-Face](Coldbrew9/Fine-tuning-gemma-singleQ)  
  
----------------------------------------------------------

# Table of Contents

1. [Introduction](#π§Hansol_llm_fine-tuning)
2. [Private μμ„](#private-μμ„)
3. [Member](#member)
4. [λ°μ΄ν„°](#λ°μ΄ν„°)
   - [train.csv](#train.csv)
   - [test.csv](#test.csv)
   - [sample_submission.csv](#sample_submission.csv)
5. [ν‰κ°€ μ‚°μ‹](#ν‰κ°€-μ‚°μ‹)
6. [μ ‘κ·Ό λ°©μ‹](#μ ‘κ·Ό-λ°©μ‹)
   - [νμΈνλ‹ μ‹λ„](#1-νμΈνλ‹-μ‹λ„)
   - [RAG(Retrieval-Augmented Generation) λ°©μ‹ μ¶”κ°€ μ‹λ„](#2-ragretrieval-augmented-generation-λ°©μ‹-μ¶”κ°€-μ‹λ„)
   - [Gemma λ¨λΈ μ‹λ„](#3-gemma-λ¨λΈ-μ‹λ„)
   - [ν•™μµ λ°μ΄ν„°μ…‹ κµ¬μ΅° μ΅°μ •](#4-ν•™μµ-λ°μ΄ν„°μ…‹-κµ¬μ΅°-μ΅°μ •)
   - [ν•μ΄νΌνλΌλ―Έν„° μ„¤μ • λ° νΉμ§•](#ν•μ΄νΌνλΌλ―Έν„°-μ„¤μ •-λ°-νΉμ§•)
7. [Wandb](#wandb)
8. [Special Thanks to](#special-thanks-to)

----------------------------------------------------------


## λ°μ΄ν„°  
![λ°μ΄ν„° ν”„λ¦¬λ·°](./pngs/data_preview.png)  
### train.csv
    id : μ§λ¬Έ - λ‹µλ³€ (QA) μƒν” κ³ μ  λ²νΈ  
    μ§λ¬Έ_1, μ§λ¬Έ_2 : μƒν” λ³„ λ™μΌν• λ‚΄μ©μΌλ΅ κµ¬μ„±λ μ§λ¬Έ 2κ°  
    category : μ§λ¬Έ - λ‹µλ³€ (QA) μƒν”μ λ„λ©”μΈ μ„Έλ¶€ λ¶„μ•Ό  
    λ‹µλ³€_1, λ‹µλ³€_2, λ‹µλ³€_3, λ‹µλ³€_4, λ‹µλ³€_5 : μƒν” λ³„ μ§λ¬Έμ— λ€ν• λ™μΌν• λ‹µλ³€ Reference 5κ°  

### test.csv
    id : ν‰κ°€ μ§λ¬Έ μƒν” κ³ μ  λ²νΈ
    μ§λ¬Έ : ν‰κ°€ μƒν”μ μ§μ λ‚΄μ©

### sample_submission.csv 
    id : ν‰κ°€ μ§λ¬Έ μƒν” κ³ μ  λ²νΈ  
    vec_0, vec_1 ... vec_511 : μƒμ„±λ λ‹µλ³€μ„ 512 μ°¨μ›μ Embedding Vectorλ΅ ν‘ν„λ κ²°κ³Ό  

## ν‰κ°€ μ‚°μ‹
    ν‰κ°€ μ‚°μ‹ : Cosine Similarity (μ½”μ‚¬μΈ μ μ‚¬λ„)
    Public score : μ „μ²΄ ν…μ¤νΈ λ°μ΄ν„° μ¤‘ μ‚¬μ „ μƒν”λ§λ 40%
    Private score : μ „μ²΄ ν…μ¤νΈ λ°μ΄ν„° 100%

    
----------------------------------------------------------

## μ ‘κ·Ό λ°©μ‹

### 1. νμΈνλ‹ μ‹λ„

- **Ko-llm-leaderboard μƒμ λ¨λΈ λΉ„κµ**
  - μ²« μ‹λ„λ΅ [Ko-llm-leaderboard](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard) μƒμ 1μ„ λ¨λΈμΈ `ldcc`λ¥Ό νμΈνλ‹ν•λ ¤ ν–μΌλ‚, `tokenizer` μ΅°μ μ— μ–΄λ ¤μ›€μ΄ μμ—μµλ‹λ‹¤.
  - μ΄ν›„ 2μ„ λ¨λΈμΈ `Edentns/DataVortexS-10.7B-dpo-v1.11`λ΅ λ°©ν–¥μ„ μ „ν™ν•μ—¬ μ‹λ„ν•΄ λ³΄μ•μµλ‹λ‹¤.

### 2. RAG(Retrieval-Augmented Generation) λ°©μ‹ μ¶”κ°€ μ‹λ„

- λ‹¤μ–‘ν• λ¬Έμ μ— λ€μ‘ν•κΈ° μ„ν•΄ RAG λ°©μ‹μ„ μ¶”κ°€λ΅ μ‹λ„ν•μ€μµλ‹λ‹¤.

### 3. Gemma λ¨λΈ μ‹λ„

- μµμ‹  μ¶μ‹λ `Gemma` λ¨λΈμ„ μ΄μ©ν• μ‹¤ν—λ„ μ§„ν–‰ν•μ€μµλ‹λ‹¤.

### 4. ν•™μµ λ°μ΄ν„°μ…‹ κµ¬μ΅° μ΅°μ •

- κΈ°μ΅΄ ν•™μµ λ°μ΄ν„°μ…‹μ€ ν• μ§λ¬Έμ— ν• λ‹µλ§ ν¬ν•¨λμ–΄ μμ—μµλ‹λ‹¤. 
- ν…μ¤νΈμ…‹μ€ λ‘ μ§λ¬Έμ„ ν• λ²μ— λ¬Όμ–΄λ³΄κ³ , λ‘ μ§λ¬Έμ— λ€ν• λ‘ κ°μ λ‹µμ„ μƒμ„±ν•΄μ•Ό ν•λ” κµ¬μ΅°μ€μµλ‹λ‹¤.
- μ΄μ— λ”°λΌ, ν•™μµμ…‹μ—μ„ μ„μλ΅ λ¬Έμ λ¥Ό λ‘ κ°μ”© ν•©μ³μ„ ν•™μµμ„ μ§„ν–‰ν•μ€μµλ‹λ‹¤.

### ν•μ΄νΌνλΌλ―Έν„° μ„¤μ • λ° νΉμ§•

λ©”λ¨λ¦¬ ν¨μ¨μ„ λ†’μ΄κΈ° μ„ν•΄ `lora rank = 8`μ„ μ‚¬μ©ν–μµλ‹λ‹¤. κ° ν•μ΄νΌνλΌλ―Έν„°μ μ„ νƒμ΄ ν•™μµ κ³Όμ •κ³Ό λ¨λΈ μ„±λ¥μ— λ―ΈμΉλ” μν–¥μ„ κ°„κ²°ν•κ² μ„¤λ…ν•©λ‹λ‹¤.

- **per_device_train_batch_size=2**: λ©”λ¨λ¦¬ μ‚¬μ© μµμ†ν™”λ¥Ό μ„ν•΄ λ””λ°”μ΄μ¤ λ‹Ή λ°°μΉ ν¬κΈ° 2λ¥Ό μ‚¬μ©.
- **gradient_accumulation_steps=4**: λ” ν° λ°°μΉ μ‚¬μ΄μ¦μ μ΄μ μ„ μ–»κΈ° μ„ν•΄ κ·Έλλ””μ–ΈνΈ λ„μ  μ¤ν… 4 μ„¤μ •.
- **warmup_ratio=0.03**: μ „μ²΄ ν•™μµ μ¤ν…μ 3% λ™μ• ν•™μµλ¥ μ„ μ μ§„μ μΌλ΅ μ¦κ°€μ‹ν‚΄.
- **num_train_epochs=10**: μ΄ 10 μ—ν­ λ™μ• λ¨λΈ ν•™μµ.
- **learning_rate=2e-4**: ν•™μµλ¥ μ„ 0.0002λ΅ μ„¤μ •ν•μ—¬ μ•μ •μ μΈ μλ ΄μ„ λ„λ¨.
- **fp16=True**: 16λΉ„νΈ λ¶€λ™ μ†μμ μ„ μ‚¬μ©ν•μ—¬ κ³„μ‚° ν¨μ¨μ„± λ° λ©”λ¨λ¦¬ μ‚¬μ© μµμ†ν™”.
- **logging_steps=1**: λ¨λ“  μ¤ν…λ§λ‹¤ λ΅κΉ…μ„ μν–‰ν•μ—¬ ν•™μµ κ³Όμ • λ¨λ‹ν„°λ§.
- **optim="paged_adamw_8bit"**: λ©”λ¨λ¦¬ μ‚¬μ©μ„ μ¤„μ΄κΈ° μ„ν•΄ 8λΉ„νΈ μµμ ν™”λ AdamW μ‚¬μ©.
- **report_to="wandb"**: ν•™μµ κ³Όμ •κ³Ό κ²°κ³Όλ¥Ό Weights & Biasesμ— κΈ°λ΅.

## Wandb
- Gemma training  
  https://wandb.ai/x_team/Fine%20tuning%20gemma%20singleQ/reports/Hansol-llm-fine-tuning-with-Gemma--Vmlldzo3MzI4MDg3

### Special Thanks to  
[μ •λ΄‰κΈ°](https://github.com/JB0527)
[μ •μ¤€ν•](https://github.com/hyjk826)
